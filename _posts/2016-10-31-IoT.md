---
layout:     post
title:      The Internet of Things
date:       2016-10-31 23:30:27
summary:    This post describes about my latest project in the IoT and big data analytics domain
categories: IoT BigData
---

Hey guys!
In this post, I'm going to discuss about an Internet of Things project that I have been working on. This project revolves around smart connected devices like smart wearables and automobile systems, and intends to leverage 'moment of truth' adverstising. I'll shortly discuss this in more detail.

Few months ago, I was made the lead to develop a prototype and then a final product version for an idea that was presented in the Hindustan Times Start-up Meet. I'm not sharing the product codebase here, but I'll share links to the repositories I made on Bitbucket and later shifted to Github while developing the prototype. Internet of Things, or as it is popularly known, IoT, refers to any and all devices that are connected to the internet. This includes smartphones, smart watches, health wearables, automobile systems, smart-tvs and more. It is estimated that there will be more than ten billion such connected devices by 2020! 

The idea for the project aims to collect data from such connected devices, starting with smart wearables, and leverage data collected by such devices. The data collected is used to implement audience segmentation and moment-of-truth advertising. For audience segmentation, I developed a Data Management Platform (DMP) that generated audiences based on filters like location, activity, measure of an activity, like running (how many km?, angle of elevation?), swimming (how many laps?, what speed?), etc. This has been done in the form of dynamic segmentation, and the audiences created could then be sold and advertised on a Demand Side Platform (DSP). Second, I built a real-time platform for moment-of-truth advertising which focused on event-driven targeting. This means that one could send an ad/alert to a user when a certain threshold value of an event is crossed. For example, send a push notification giving the location of nearest refreshmant store to every user who ran 10 km or more, or give the traffic and weather updates based on the location of a frequent jogger. The campaign to create such an event and it's corresponding threshold value is set from the admin dashboard.

I'll elaborately discuss the problem statement, the technology used and the codebase in the following segments:

<b>Data Collection:</b>

  For the project, we built a native SDK, i.e., a software development toolkit, on Android and iOS platforms that could be integrated with various apps and smart devices. The SDK is built to collect data like device id, advertising id, device info (parameters like manufacturer, sensor data), events triggered in the device and more. The format of the data is in JSON (Javascript Object Notation), and in accordance with a pre-defined schema format. The data is sent as a packet on the nginx server. I have already described the purpose, setup and installation of nginx in a separate post. Nginx acts as a load balancer and shades the backend system. The repository for the nginx configuration and files can be found at: <a href="https://github.com/sahiltyagi4/iot-conf"><i>https://github.com/sahiltyagi4/iot-conf</i></a>.

  The repository for node server can be found at https://github.com/sahiltyagi4/nodeServer.
  
  Nginx redirects the incoming packets to Node.js code, listening on port 3000 (as specified in the configuration file production_conf.json). The main piece of code, index.js, found at https://github.com/sahiltyagi4/nodeServer/blob/master/index.js, listens on port 3000 and appends additional data like server IP, library version of the SDK, server timestamp, etc. It then serializes the incoming json packets into Avro objects. The usage of Apache Avro serves dual purpose: it serializes the data to be interpreted by different programming languages, and it ensures the incoming packets adhere to the predefined schema. The schema I designed goes like https://github.com/sahiltyagi4/iot-conf/blob/master/iot_data_schema.avsc. These avro objects are then saved into kafka, under a topic name specified by production_conf.json.

<b>Data Ingestion and Processing:</b>

  At this point, the data collected by the sdk is sent over to the nginx server, which redirects it to the Node.js server. Some additional data is appended to the json and it is converted into Avro objects. Finally, these avro objects are dumped into kafka. The data is stored over Hadoop Distributed File System (HDFS), Hortonworks version, the setup and installation of which is in a separate post.

  The data ingestion and processing is further divided into two segments: streaming layer (or speed layer or real-time layer) and batch layer.

  <b>Streaming Layer:</b>
    
   The repository where I have Spark Streaming running is at https://github.com/sahiltyagi4/iot_sparkStreaming.

   To ingest and process the data stored in kafka, I used Spark v1.6 and setup a streaming job to act as a consumer. The main java class that does the job is https://github.com/sahiltyagi4/iot_sparkStreaming/blob/master/src/main/java/com/iot/data/stream/IotDataStreamer.java. The data from the incoming stream is stored in Hbase. The advantage of storing the data in Hbase over HDFS is that it is efficient to retrieve and implicitly handles the small file problem. The data is ultimately stored back into HDFS int the form of H Files. The data is stored in two tables: iot_primary and iot_secondary. The Hbase table iot_primary is the primary dataset or the master dataset. It contains every data point of every packet that has been registered. The same data goes into another Hbase table, iot_secondary which is used for the batch processing task. The data in iot_secondary gets deleted after being processed. The property of Hbase is that it works as a key-value store. This means a key must be unique in a given Hbase table. Any value with a key that already exists in the table will be overriden by a new value with the same key. Hence, one must ensure the uniqueness of every record that goes into a table. In the given problem statement, the row key in iot_primary and iot_secondary is a combination of the app secret (the id of every app in which the SDK is installed) and packet id (an 8-bit randomly generated id).

   Another application of the Streaming Layer in IoT is triggered push notification. This makes moment-of-truth advertising possible. When I say moment-of-truth, I mean the ability to reach a user/customer whenever a ceratin threshold value of an event is crossed. I integrated other third party APIs like google traffic and weather forecast, to make more intelligent recommendations. For example, send an ad giving the location of the nearest Starbucks to a user who finishes running 10 kilometers, or give the weather and traffic updates to a frequent jogger who is currently on a run. The way to showcase ads that has been implemented in the prototype is through push notifications in the app that has the native SDK embedded in it.

   The facility to create a campaign has been provided on the web dashboard, where the advertiser/app owner can select an event, the corresponding threshold value at which it should be triggered and the frequency for which a push should be sent to a single user. For example, set a notification that is triggered whenever a user finishes running 10 kilometers or more. Here, the event name is 'run', and the event count is '10 km' and frequency is set to 3. So, the system will send a push notification giving the location (lat/long) of the nearest Starbucks restaurant for an energy drink. Various third-party data has been integrated int the triggered push system like Google Places, Google Traffic and Weather Forecast. The push system inherently needs a push server, which sends the pushkey (a unique id alloted by Google to identify a device for an app on android) to the Google Cloud Messaging (GCM) client with the lat/long data. The GCM then sends the push to the corresponding device. When user clicks the push notification, s/he is redirected to Google Maps showing the shortest route to the nearest Starbucks store.

   I'm working on taking this further, to include Google, Bing and Facbook Ads as various other advertising platforms. This is possible since the native SDK is also providing us the advertising id, or as it's popularly called, the ad id of the user.

   This real-time, moment-of-truth advertising is happening in the Spark Streaming job that is running on a cluster of nodes. There is a filter RDD (resilient distributed datset) that scans for the event and event count corresponding to which a triggered campaign has been set. When the ideal packet comes in, it extracts the device id and pushkey, and forwards it to the push server, which finally sends data across through GCM client.


  <b>Batch Layer:</b>
		
   The second aspect of the system architecture is the batch layer. This layer runs the bulky and batch processing tasks like finding the active users, new users, event-wise count, event pivoting, etc. I will explain each of these in more detail. The computing engine for batch processing currently deployed is Spark v2.0.1.
   The repository for the batch layer is https://github.com/sahiltyagi4/iot_spark2.0.

   The main class running the Spark job for batch processing is https://github.com/sahiltyagi4/iot_spark2.0/blob/master/src/main/java/com/iot/data/queries/BatchProcess.java.
   The batch processing starts from the iot_secondary table. The data from this Hbase table is read and every record is saved into two separate tables: activeUsersPreprocess and newUsersPreprocess. These tables contain data like app secret, device id and timestamp, and help in calculating the active and new users corresponding to every app on a segmented basis. The iot_secondary table is read and the selected data points from every packet are saved in newUsersPreprocess. Then the same is done for activeUsersPreprocess. The total number of active and new users corresponding to different apps are then calculated accordingly. A mapping of push key corresponding to every app secret and device id is also kept. After completion of active and new user queries, we can see the beginning of the execution of event-wise queries in the code. This includes queries like the total event count, event-wise active users and event-wise new users, events pivoting and more. Events pivoting aims to find correlation among different events in massive data sets. For example, of all the users who are running, how many and who all are swimming as well? This helps in creating a semi-segmented audience. At the end of batch processing, all the records in iot_secondary that have been processed are then deleted, and loop goes back on and on.


<b>Near Real-Time (NRT) Layer:</b>

  I am currently exploring near real-time and on-the-fly computing mecahnisms like ElasticSearch. The version I'm presently experimenting with is v2.4.0. The objective of the research is to be able to create audiences of users based on a wide number of variables and forward the same to a DSP. For example, I might want to identify the users who jog in Gurgaon, Haryana between 10:00 am to 6:00 pm, or create an audience which swims more than 5 laps on a daily basis. This is a part of what is known as Dynamic Segmentation. ElasticSearch is built over Lucene which is based on the concept of Inverted Index and so far, it has delivered all that it promised!


So this was a slight overview of what I have been doing over the past few months. You may find a lot more code in the repositories than actually implemented so far. I will add the respective parts in coming releases. Other than that, I am exploring Structured Streaming which I later plan to implement once the beta version rolls out. This would empower developers the ability to perform batch processing operations (Dataframe/Dataset, Spark-SQL, etc) in the streaming application. I am also working on making the entire system more intelligent by creating a profile of users based on the historical aggregation of all their events/activities. To make this possible in distributed processing, we have Spark MLlib to the rescue!

So that's all there is for today. I will try to upload my other ongoing projects and explorations as frequently as I can.
In case you have any queries or want to know/discuss in deeper detail, please feel free to drop in a mail to sahilt.tyagi@gmail.com. Brainstorming sessions are always welcome..

Cheers :)
