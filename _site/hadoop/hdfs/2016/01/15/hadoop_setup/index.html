<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Hadoop and HDFS setup &#8211; Sahil Tyagi</title>
    <link rel="dns-prefetch" href="//maxcdn.bootstrapcdn.com">
    <link rel="dns-prefetch" href="//cdn.mathjax.org">
    <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="This post describes how to setup Hadoop 2.2.3 (Hortonworks release) on an Ubuntu (Linux) server.">
    <meta name="robots" content="all">
    <meta name="author" content="Sahil Tyagi">
    <meta name="keywords" content="hadoop, hdfs">
    <link rel="canonical" href="//hadoop/hdfs/2016/01/15/hadoop_setup/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Sahil Tyagi" href="/feed.xml" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?201608102009" type="text/css">

    <!-- Fonts -->
    
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
    

    <!-- MathJax -->
    

    <!-- Verifications -->
    
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Hadoop and HDFS setup">
    <meta property="og:description" content="A simple, beautiful theme for Jekyll that emphasizes content rather than aesthetic fluff.">
    <meta property="og:url" content="//hadoop/hdfs/2016/01/15/hadoop_setup/">
    <meta property="og:site_name" content="Sahil Tyagi">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />
    
    <meta name="twitter:title" content="Hadoop and HDFS setup" />
    <meta name="twitter:description" content="This post describes how to setup Hadoop 2.2.3 (Hortonworks release) on an Ubuntu (Linux) server." />
    <meta name="twitter:url" content="//hadoop/hdfs/2016/01/15/hadoop_setup/" />

    <!-- Icons -->
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-192x192.png" sizes="192x192">
    <link rel="icon" type="image/png" href="/favicon-160x160.png" sizes="160x160">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">

    
</head>

<body class="site">
  
	

  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <a href="/" class="site-title">Sahil Tyagi</a>
      <nav class="site-nav">
        
    

    
        <a href="/about/">About Me</a>
    

    

    

    

    

    


    

    

    
        <a href="/contact/">Say Hello</a>
    

    

    

    

    


      </nav>
      <div class="clearfix"></div>
      
    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        


<div class="post-header mb2">
  <h1>Hadoop and HDFS setup</h1>
  <span class="post-meta">Jan 15, 2016</span><br>
  
  <!-- <span class="post-meta small">
  
    10 minute read
  
  </span> -->
</div>

<article class="post-content">
  <p>Hadoop, formally called Apache Hadoop, is an Apache Software Foundation project and open source software platform for scalable, distributed computing. It provides massive storage for any kind of data, enormous processing power and the ability to handle virtually limitless concurrent tasks or jobs. 
Hadoop is designed to run on a large number of machines that don’t share any memory or disks. That means you can buy a whole bunch of commodity servers, slap them in a rack, and run the Hadoop software on each one. When you want to load all of your organization’s data into Hadoop, what the software does is bust that data into pieces that it then spreads across your different servers. There’s no one place where you go to talk to all of your data; Hadoop keeps track of where the data resides. And because there are multiple copy stores, data stored on a server that goes offline or dies can be automatically replicated from a known good copy. Hadoop is ideal for situations where you want to run analytics that are deep and computationally extensive, like clustering and targeting. It applies to a myriad of markets like finance, online retail, etc.
Hadoop 2.x comes with YARN (Yet Another Resource Negotiator) for cluster resource management.
Following are the installation steps:</p>

<ul>
  <li>
    <p>Check the ip of the server by running the command:</p>

    <p><code class="highlighter-rouge">
  $ ifconfig
 </code></p>

    <p>Let’s say the ip looks somewhat like xxx.xxx.xx.xxx</p>
  </li>
  <li>
    <p>Check the hostname provided to the server by running nslookup on the remote machine:</p>

    <p><code class="highlighter-rouge">
  $ nslookup xxx.xxx.xx.xxx
 </code></p>
  </li>
  <li>
    <p>SSH to the server from your local machine:</p>

    <p><code class="highlighter-rouge">
  $ ssh root@xxx.xxx.xx.xxx
 </code></p>
  </li>
</ul>

<p>You should see something like the following after entering credentials:
	root@Ubuntu-1204-precise-64-minimal ~</p>

<ul>
  <li>
    <p>Edit /etc/hostname :</p>

    <p><code class="highlighter-rouge">
   vim /etc/hostname
  </code></p>
  </li>
  <li>
    <p>Change all instances of the current  hostname (Ubuntu-1204-precise-64-minimal) to the Hostname provided and save the file.
  If everything goes well, you should see an entry in /etc/hosts like the one shown below.
  xxx.xxx.xx.xxx 	static.xxx.xxx.xx.xxx.clients.your-server.de</p>

    <p>If you don’t see such an entry, add it manually.</p>
  </li>
  <li>
    <p>Edit <b>/etc/hosts</b> :</p>

    <p><code class="highlighter-rouge">
  $ vim /etc/hosts
 </code></p>

    <p>(If its empty just enter machine name)</p>
  </li>
  <li>
    <p>Again change the hostname with $ vim /etc/hostname command</p>
  </li>
  <li>
    <p>Reboot the machine:</p>

    <p><code class="highlighter-rouge">
  $ sudo reboot
 </code></p>
  </li>
  <li>
    <p>Download the .tar.gz file of the latest JDK from <b>http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.htm
  lwget –no-cookies –header “Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com” “http://download.oracle.com/otn-pub/java/jdk/7u51-b13/jdk-7u51-linux-x64.tar.gz”</b></p>

    <p><b>UPDATE</b> :</p>

    <p><b>NOTE-1</b> : The version mentioned here is not the latest version of java. If you want to download the latest version, get the appropriate .tar.gz url. If you want the 7U51 version (used in the document) get the new link from archives page.</p>

    <p><b>NOTE-2</b> : if the wget command used above doesn’t work, use <b>wget –no-check-certificate –no-cookies –header “Cookie:oraclelicense=accept-securebackup-cookie”</b> instead of 
  <b>wget –no-cookies –header “Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com”</b> on your local machine.</p>
  </li>
  <li>
    <p>Create a folder for the java setup on the remote machine:</p>

    <p><code class="highlighter-rouge">
  $ mkdir /usr/local/java
  $ cd /usr/local/java
 </code></p>
  </li>
  <li>
    <p>Copy the java setup to remote machine by running the following command on your local machine :</p>

    <p><code class="highlighter-rouge">
  $ scp jdk-7u51-linux-x64.tar.gz root@148.xxx.xx.135:/usr/local/java/
 </code></p>
  </li>
  <li>
    <p>Run the following command (replace jdk-7u51 with the corresponding version name) on the remote machine:</p>

    <p><code class="highlighter-rouge">
  $ cd /usr/local/java
  $ sudo tar -xvzf jdk-7u51-linux-x64.tar.gz
 </code></p>
  </li>
  <li>
    <p>Check the name of the extracted folder (marked in bold, this may differ for you, and you need to use that particular name in the steps that follow):</p>

    <p><code class="highlighter-rouge">
  root@hadoop-namenode /usr/local/java  ls -a
      .  ..  jdk1.7.0_51  jdk-7u51-linux-x64.tar.gz
 </code></p>
  </li>
  <li>
    <p>(Optional) At this point, you can remove the .tar.gz file</p>

    <p><code class="highlighter-rouge">
  rm jdk-7u51-linux-x64.tar.gz
 </code></p>
  </li>
  <li>
    <p>$ <b>vim /etc/profile</b></p>
  </li>
  <li>
    <p>Add the following lines at the end of the file, and save the file (Esc :wq):</p>

    <p><code class="highlighter-rouge">
  JAVA_HOME=/usr/local/java/jdk1.7.0_51
  PATH=$PATH:$HOME/bin:$JAVA_HOME/bin
  export JAVA_HOME
  export PATH
 </code></p>
  </li>
  <li>
    <p>Inform the system where your Oracle Java JDK is located. This will tell the system that the new Oracle Java version is available for use.</p>

    <p><code class="highlighter-rouge">
  root@hadoop-namenode /usr/local/java  sudo update-alternatives --install "/usr/bin/java" "java" "/usr/local/java/jdk1.7.0_79/bin/java" 1
 </code></p>

    <p><code class="highlighter-rouge">
  root@hadoop-namenode /usr/local/java  sudo update-alternatives --install "/usr/bin/javac" "javac" "/usr/local/java/jdk1.7.0_51/bin/javac" 1
 </code></p>

    <p><code class="highlighter-rouge">
  root@hadoop-namenode /usr/local/java  sudo update-alternatives --install "/usr/bin/javaws" "javaws" "/usr/local/java/jdk1.7.0_51/bin/javaws" 1
 </code></p>
  </li>
  <li>
    <p>Inform the system that Oracle Java JDK must be the default Java.</p>

    <p><code class="highlighter-rouge">
  root@hadoop-namenode /usr/local/java  sudo update-alternatives --set java /usr/local/java/jdk1.7.0_79/bin/java
 </code></p>

    <p><code class="highlighter-rouge">
  root@hadoop-namenode /usr/local/java  sudo update-alternatives --set javac /usr/local/java/jdk1.7.0_51/bin/javac
 </code></p>

    <p><code class="highlighter-rouge">
  root@hadoop-namenode /usr/local/java  sudo update-alternatives --set javaws /usr/local/java/jdk1.7.0_51/bin/javaws
 </code></p>
  </li>
  <li>
    <p>Reload your system wide PATH <b>/etc/profile</b> :</p>

    <p><code class="highlighter-rouge">
  root@hadoop-namenode /usr/local/java  . /etc/profile
 </code></p>
  </li>
  <li>
    <p>Check that java was installed correctly:</p>

    <p><code class="highlighter-rouge">
  root@hadoop-namenode ~  java -version
      java version "1.7.0_51"
      Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
      Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)
 </code></p>

    <p><code class="highlighter-rouge">
  root@hadoop-namenode ~  javac -version
      javac 1.7.0_51
 </code></p>
  </li>
  <li>
    <p>Install NTP (if not already installed):</p>

    <p><code class="highlighter-rouge">
  root@hadoop-namenode ~  apt-get install ntp
 </code></p>
  </li>
  <li>
    <p><b>sudo /etc/init.d/ntp reload</b></p>
  </li>
  <li>
    <p>Disable SELinux:</p>

    <p><code class="highlighter-rouge">
   apt-get install selinux-utils
   getenforce
  </code></p>

    <p>If result is disabled or permissive, no further action is required. Otherwise</p>

    <p><code class="highlighter-rouge">
   setenforce 0
  </code></p>
  </li>
  <li>
    <p>Disable IPTables:</p>

    <p><code class="highlighter-rouge">
   service ufw stop
  </code></p>
  </li>
  <li>
    <p>Install requirements:</p>

    <p><code class="highlighter-rouge">
   sudo apt-get install rpm
   sudo apt-get install curl
  </code></p>
  </li>
  <li>
    <p>Configure remote repositories and add gpg keys:</p>

    <p><code class="highlighter-rouge">
  wget http://public-repo-1.hortonworks.com/HDP/ubuntu12/2.x/hdp.list -O /etc/apt/sources.list.d/hdp.list
 </code></p>

    <p><code class="highlighter-rouge">
  wget http://public-repo-1.hortonworks.com/HDP/ubuntu12/2.0.6.1/hdp.list -O /etc/apt/sources.list.d/hdp.list
 </code></p>

    <p><code class="highlighter-rouge">
   gpg --keyserver pgp.mit.edu --recv-keys B9733A7A07513CAD
   gpg -a --export 07513CAD | apt-key add -
   apt-get update
  </code></p>
  </li>
  <li>
    <p>Download companion files</p>

    <p><code class="highlighter-rouge">
   mkdir /tmp/hadoop
   cd /tmp/hadoop
   wget http://public-repo-1.hortonworks.com/HDP/tools/2.1.5.0/hdp_manual_install_rpm_helper_files-2.1.5.695.tar.gz
   tar -zxvf hdp_manual_install_rpm_helper_files-2.0.6.101.tar.gz
   cd hdp_manual_install_rpm_helper_files-2.0.6.101/
  </code></p>
  </li>
  <li>
    <p><b>cd scripts</b></p>

    <p><code class="highlighter-rouge">
 vim usersAndGroups.sh
</code></p>

    <p>Comment the variables (using ) corresponding to the services that are not being installed (from PIG_USER to OOZIE_USER) and save the file (Esc :wq)</p>

    <p><code class="highlighter-rouge">
 source usersAndGroups.sh
</code></p>
  </li>
  <li>
    <p><b>vim directories.sh</b></p>

    <div class="highlighter-rouge"><pre class="highlight"><code>  #Set the following variables and comment the rest:

  DFS_NAME_DIR="/grid/hadoop/hdfs/nn";

  DFS_DATA_DIR="/grid/hadoop/hdfs/dn";

  FS_CHECKPOINT_DIR="/grid/hadoop/hdfs/snn";

  HDFS_LOG_DIR="/var/log/hadoop/hdfs";

  HDFS_PID_DIR="/var/run/hadoop/hdfs";

  HADOOP_CONF_DIR="/etc/hadoop/conf";

  YARN_LOCAL_DIR="/grid/hadoop/yarn/local";

  YARN_LOG_DIR="/var/log/hadoop/yarn";

  YARN_LOCAL_LOG_DIR="/grid/hadoop/yarn/logs";

  YARN_PID_DIR="/var/run/hadoop/yarn";

  MAPRED_LOG_DIR="/var/log/hadoop/mapred";

  MAPRED_PID_DIR="/var/run/hadoop/mapred";

  export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec
</code></pre>
    </div>

    <p>Save the file and source it.</p>

    <p><b>source directories.sh</b></p>
  </li>
  <li>
    <p><b>vim ~/hadoopVariables.sh</b></p>

    <p>#Add the following</p>

    <div class="highlighter-rouge"><pre class="highlight"><code> !/bin/sh

 #Hadoop Users and Groups
 #User which will own the HDFS services.
 HDFS_USER=hdfs ;

 #User which will own the YARN services.
 YARN_USER=yarn ;

 #User which will own the MapReduce services.
 MAPRED_USER=mapred ;

 #A common group shared by services.
 HADOOP_GROUP=hadoop ;

 #Hadoop Service - HDFS
 #Space separated list of directories where NameNode will store file system image. For example, /grid/hadoop/hdfs/nn /grid1/hadoop/hdfs/nn

 DFS_NAME_DIR="/grid/hadoop/hdfs/nn";

 #Space separated list of directories where DataNodes will store the blocks. For example, /grid/hadoop/hdfs/dn /grid1/hadoop/hdfs/dn /grid2/hadoop/hdfs/dn

 DFS_DATA_DIR="/grid/hadoop/hdfs/dn";

 #Space separated list of directories where SecondaryNameNode will store checkpoint image. For example, /grid/hadoop/hdfs/snn /grid1/hadoop/hdfs/snn /grid2/hadoop/hdfs/snn

 FS_CHECKPOINT_DIR="/grid/hadoop/hdfs/snn";

 #Directory to store the HDFS logs.
 HDFS_LOG_DIR="/var/log/hadoop/hdfs";

 #Directory to store the HDFS process ID.
 HDFS_PID_DIR="/var/run/hadoop/hdfs";

 #Directory to store the Hadoop configuration files.
 HADOOP_CONF_DIR="/etc/hadoop/conf";

 #Hadoop Service - YARN 
 #Space separated list of directories where YARN will store temporary data. For example, /grid/hadoop/yarn/local /grid1/hadoop/yarn/local /grid2/hadoop/yarn/local

 YARN_LOCAL_DIR="/grid/hadoop/yarn/local";

 #Directory to store the YARN logs.
 YARN_LOG_DIR="/var/log/hadoop/yarn"; 

 #Space separated list of directories where YARN will store container log data. For example, /grid/hadoop/yarn/logs /grid1/hadoop/yarn/logs /grid2/hadoop/yarn/logs

 YARN_LOCAL_LOG_DIR="/grid/hadoop/yarn/logs";

 #Directory to store the YARN process ID.
 YARN_PID_DIR="/var/run/hadoop/yarn";

 #Hadoop Service - MAPREDUCE
 #Directory to store the MapReduce daemon logs.
 MAPRED_LOG_DIR="/var/log/hadoop/mapred";

 #Directory to store the mapreduce jobhistory process ID.
 MAPRED_PID_DIR="/var/run/hadoop/mapred";

 export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec
</code></pre>
    </div>
  </li>
  <li>
    <p><b>chmod +x hadoopVariables.sh</b></p>

    <p><code class="highlighter-rouge">
   source hadoopVariables.sh
  </code></p>
  </li>
  <li>
    <p><code class="highlighter-rouge">
  python yarn-utils.py -c 6 -m 128 -d 1 -k False
 </code></p>

    <p>Replace 6 by no.of cores, 128 by no. of gigabytes of RAM and 1 by no. of disks.Save the ouput for use in step 42 (yarn-site.xml)</p>
  </li>
  <li>
    <p><b>umask</b>
  0022</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">
  sudo apt-get install openssh-server
 </code></p>
  </li>
  <li>
    <p>Install hadoop core services:</p>

    <p><code class="highlighter-rouge">
   sudo apt-get install hadoop hadoop-hdfs libhdfs0 libhdfs0-dev hadoop-yarn hadoop-mapreduce hadoop-client openssl
  </code></p>
  </li>
  <li>
    <p>Install compression libraries</p>

    <p><code class="highlighter-rouge">
  apt-get install libsnappy1 libsnappy-dev
 </code></p>

    <p><code class="highlighter-rouge">
  ln -sf /usr/lib64/libsnappy.so /usr/lib/hadoop/lib/native/.
 </code></p>

    <p><code class="highlighter-rouge">
  apt-get install liblzo2-2 liblzo2-dev hadoop-lzo
 </code></p>
  </li>
  <li>
    <p>Create the NameNode Directories (ONLY on NameNode):</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>  mkdir -p $DFS_NAME_DIR;
  chown -R $HDFS_USER:$HADOOP_GROUP $DFS_NAME_DIR;
  chmod -R 755 $DFS_NAME_DIR;
</code></pre>
    </div>
  </li>
  <li>
    <p>Create the Secondary NameNode Directories (ONLY on SecondaryNameNode):</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>  mkdir -p $FS_CHECKPOINT_DIR;
  chown -R $HDFS_USER:$HADOOP_GROUP $FS_CHECKPOINT_DIR;
  chmod -R 755 $FS_CHECKPOINT_DIR;
</code></pre>
    </div>
  </li>
  <li>
    <p>ONLY on all the datanodes :</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>  mkdir -p $DFS_DATA_DIR;
  chown -R $HDFS_USER:$HADOOP_GROUP $DFS_DATA_DIR;
  chmod -R 750 $DFS_DATA_DIR;
</code></pre>
    </div>
  </li>
  <li>
    <p>ONLY on ResourceManager and all datanodes :</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>  mkdir -p $YARN_LOCAL_DIR;
  chown -R $YARN_USER:$HADOOP_GROUP $YARN_LOCAL_DIR;
  chmod -R 755 $YARN_LOCAL_DIR;
  mkdir -p $YARN_LOCAL_LOG_DIR;
  chown -R $YARN_USER:$HADOOP_GROUP $YARN_LOCAL_LOG_DIR;
  chmod -R 755 $YARN_LOCAL_LOG_DIR;
</code></pre>
    </div>
  </li>
  <li>
    <p>On all nodes:</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>  mkdir -p $HDFS_LOG_DIR;
  chown -R $HDFS_USER:$HADOOP_GROUP $HDFS_LOG_DIR;
  chmod -R 755 $HDFS_LOG_DIR;
  mkdir -p $YARN_LOG_DIR;
  chown -R $YARN_USER:$HADOOP_GROUP $YARN_LOG_DIR;
  chmod -R 755 $YARN_LOG_DIR;
  mkdir -p $HDFS_PID_DIR;
  chown -R $HDFS_USER:$HADOOP_GROUP $HDFS_PID_DIR;
  chmod -R 755 $HDFS_PID_DIR;
  mkdir -p $YARN_PID_DIR;
  chown -R $YARN_USER:$HADOOP_GROUP $YARN_PID_DIR;
  chmod -R 755 $YARN_PID_DIR;
  mkdir -p $MAPRED_LOG_DIR;
  chown -R $MAPRED_USER:$HADOOP_GROUP $MAPRED_LOG_DIR;
  chmod -R 755 $MAPRED_LOG_DIR;
  mkdir -p $MAPRED_PID_DIR;
  chown -R $MAPRED_USER:$HADOOP_GROUP $MAPRED_PID_DIR;
  chmod -R 755 $MAPRED_PID_DIR;
</code></pre>
    </div>
  </li>
  <li>
    <p><b>cd /tmp/hadoop/hdp_manual_install_rpm_helper_files-2.0.6.101/configuration_files/core_hadoop</b></p>
  </li>
  <li>
    <p><b>vim core-site.xml</b></p>

    <div class="highlighter-rouge"><pre class="highlight"><code>  &lt;!--Make the following change--&gt;
 	&lt;property&gt;
  	&lt;name&gt;fs.defaultFS&lt;/name&gt;
 	&lt;value&gt;hdfs://static.xxx.xxx.xx.xxx.clients.your-server.de:8020&lt;/value&gt;
 	&lt;/property&gt;
</code></pre>
    </div>
  </li>
  <li>
    <p><b>vim hdfs-site.xml</b></p>

    <div class="highlighter-rouge"><pre class="highlight"><code>  &lt;!--Make the following changes--&gt;
  &lt;property&gt;
  	&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
  	&lt;value&gt;file:///grid/hadoop/hdfs/dn&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
 	&lt;name&gt;dfs.namenode.http-address&lt;/name&gt;
  	&lt;value&gt;static.xxx.xxx.xx.xxx.clients.your-server.de:50070&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
  	&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
  	&lt;value&gt;file:///grid/hadoop/hdfs/nn&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
  	&lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;
  	&lt;value&gt;/grid/hadoop/hdfs/snn&lt;/value&gt;
  	&lt;/property&gt;
  &lt;property&gt;
  	&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
  	&lt;value&gt;static.xxx.xxx.xx.xxx.clients.your-server.de:50090&lt;/value&gt;
  &lt;/property&gt;
</code></pre>
    </div>
  </li>
  <li>
    <p><b>vim yarn-site.xml</b></p>

    <div class="highlighter-rouge"><pre class="highlight"><code>  &lt;!--Make the following changes--&gt;
  	&lt;property&gt;
  		&lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;
  		&lt;value&gt;/grid/hadoop/yarn/local&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;yarn.log.server.url&lt;/name&gt;
  		&lt;value&gt;http://static.xxx.xxx.xx.xxx.clients.your-server.de:19888/jobhistory/logs&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
  		&lt;value&gt;static.xxx.xxx.xx.xxx.clients.your-server.de:8141&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;
  		&lt;value&gt;105984&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
  		&lt;value&gt;105984&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;
  		&lt;value&gt;/grid/hadoop/yarn/logs&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;
  		&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
  		&lt;value&gt;static.xxx.xxx.xx.xxx.clients.your-server.de:8025&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;
  		&lt;value&gt;35328&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
  		&lt;value&gt;static.xxx.xxx.xx.xxx.clients.your-server.de:8088&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
  		&lt;value&gt;static.xxx.xxx.xx.xxx.clients.your-server.de:8050&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
  		&lt;value&gt;static.xxx.xxx.xx.xxx.clients.your-server.de:8030&lt;/value&gt;
	&lt;/property&gt;
</code></pre>
    </div>
  </li>
  <li>
    <p><b>vim mapred-site.xml</b></p>

    <div class="highlighter-rouge"><pre class="highlight"><code>  &lt;!--Make the following changes--&gt;
  	&lt;property&gt;
  		&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
  		&lt;value&gt;static.xxx.xx.xxx.xxx.clients.your-server.de:10020&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;mapreduce.cluster.reduce.memory.mb&lt;/name&gt;
  		&lt;value&gt;35328&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;mapred.child.java.opts&lt;/name&gt;
  		&lt;value&gt;-Xmx28262m&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;mapred.cluster.max.reduce.memory.mb&lt;/name&gt;
  		&lt;value&gt;35328&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;mapred.cluster.max.map.memory.mb&lt;/name&gt;
  		&lt;value&gt;35328&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
  		&lt;value&gt;static.xxx.xx.xxx.xxx.clients.your-server.de:19888&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;
  		&lt;value&gt;35328&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;mapreduce.task.io.sort.mb&lt;/name&gt; 
  		&lt;!--Please keep this as 1024 --&gt;
  		&lt;value&gt;1024&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;mapred.cluster.map.memory.mb&lt;/name&gt;
  		&lt;value&gt;35328&lt;/value&gt;
	&lt;/property&gt;
  	&lt;property&gt;
  		&lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;
  		&lt;value&gt;35328&lt;/value&gt;
	&lt;/property&gt;
</code></pre>
    </div>
  </li>
  <li>
    <p>Copy the configuration files to <b>/etc/hadoop/conf/</b></p>
  </li>
  <li>
    <p><b>vim /etc/hadoop/conf/hadoop-env.sh</b></p>

    <p>Comment the line containing export JAVA_HOME</p>
  </li>
  <li>
    <p><b>vim /etc/rc.local</b></p>

    <div class="highlighter-rouge"><pre class="highlight"><code>  Add the following lines (after the first block of comments):
  mkdir /var/run/hadoop
  chown -R hdfs:hadoop /var/run/hadoop
  chmod -R 755 /var/run/hadoop

  mkdir /var/run/hadoop-yarn
  chown -R yarn:hadoop /var/run/hadoop-yarn
  chmod -R 755 /var/run/hadoop-yarn

  mkdir /var/run/hadoop-mapreduce
  chown -R mapred:hadoop /var/run/hadoop-mapreduce
  chmod -R 755 /var/run/hadoop-mapreduce
</code></pre>
    </div>

    <p>(Need to use distributed cache) Copy all the lib jars from <b>/home/hadoop/rocq/spahadoop_backend/lib</b> to <b>/usr/lib/hadoop-mapreduce/</b>
  ** add <b>/usr/lib/rocq</b> folder &amp; <b>/usr/lib/hadoop-mapreduce/</b> folder jars while adding new datanodes or any other node to the cluster</p>
  </li>
  <li>
    <p>ONLY on the namenode, format and start the namenode</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>su $HDFS_USER
hdfs namenode -format
</code></pre>
    </div>

    <p><code class="highlighter-rouge">
   /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start namenode
 </code></p>

    <p>If the above commands execute properly, then the namenode setup is working.
  When namenode is running, see : http://static.136.xx.xxx.148.clients.your-server.de:50070/</p>
  </li>
  <li>
    <p>Execute these commands on the SecondaryNameNode:</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>su $HDFS_USER
</code></pre>
    </div>

    <p><code class="highlighter-rouge">
   /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start secondarynamenode
  </code></p>
  </li>
  <li>
    <p>Execute these commands on all DataNodes:</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>su $HDFS_USER
</code></pre>
    </div>

    <p><code class="highlighter-rouge">
   /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start datanode
  </code></p>
  </li>
  <li>
    <p>Create hdfs user directory in HDFS:</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>su $HDFS_USER
hadoop fs -mkdir -p /user/hdfs
</code></pre>
    </div>

    <p>Try copying a file into HDFS and listing that file:</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>su $HDFS_USER
hadoop fs -copyFromLocal /etc/passwd passwd
hadoop fs -ls 
</code></pre>
    </div>

    <p>If  you see the file listed, then it has been created properly.</p>
  </li>
  <li>
    <p>Start YARN
  Execute these commands from the ResourceManager server:</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>su $YARN_USER
export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec
</code></pre>
    </div>

    <p><code class="highlighter-rouge">
  /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start resourcemanager
 </code></p>

    <p>Execute these commands from all NodeManager nodes:</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>su $YARN_USER
export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec
</code></pre>
    </div>

    <p><code class="highlighter-rouge">
   /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager
  </code></p>
  </li>
  <li>
    <p>Start MapReduce JobHistory Server (on the namenode):
  We need to do this step on data-nodes before starting node manager, else it shows error on health check and shows unhealthy nodes.</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>chown -R root:hadoop /usr/lib/hadoop-yarn/bin/container-executor
chmod -R 6050 /usr/lib/hadoop-yarn/bin/container-executor
	
$ su hdfs
$ hadoop fs -mkdir -p /mr-history/tmp
$ hadoop fs -chmod -R 1777 /mr-history/tmp
$ hadoop fs -mkdir -p /mr-history/done
$ hadoop fs -chmod -R 1777 /mr-history/done
$ hadoop fs -chown -R mapred:hdfs /mr-history
$ hadoop fs -mkdir -p /app-logs
$ hadoop fs -chmod -R 1777 /app-logs 
$ hadoop fs -chown yarn /app-logs
$ exit


$ su mapred
$ export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec/ 
$ /usr/lib/hadoop-mapreduce/sbin/mr-jobhistory-daemon.sh --config /etc/hadoop/conf start historyserver
</code></pre>
    </div>
  </li>
  <li>
    <p>Browse the Recource Manager :</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>http://static.136.xx.xxx.148.clients.your-server.de:8088/
</code></pre>
    </div>
  </li>
  <li>
    <p>Start a sample MapReduce Job:</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>su hdfs
$ /usr/lib/hadoop/bin/hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples-2.2.0.2.0.6.0-101.jar teragen 10000 /tmp/teragenout 
$ /usr/lib/hadoop/bin/hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples-2.2.0.2.0.6.0-76.jar terasort /tmp/teragenout /tmp/terasortout
</code></pre>
    </div>
  </li>
  <li>
    <p>If there are no errors, then the setup is working!</p>
  </li>
  <li>
    <p>For stopping follow this order</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>Job History Server
NodeManager
ResourceManager
DataNodes
SecondaryNameNode
NameNode
</code></pre>
    </div>
  </li>
</ul>

</article>











      </div>
    </div>
  </div>

  <footer class="center">
  <div class="measure">
    <small>
     Thanks for visiting!
    </small>
  </div>
</footer>


</body>
</html>
